\chapter{Concurrent Multiscale Simulations of Nonlinear Random Materials Using Probabilistic Learning}
\label{chap:fe2}

 In this work, we explore an alternative path to address this problem and seek to construct a statistical surrogate model where the forward map of interest (specifically, the non-local constitutive model) is approximated using statistical conditioning. Instead of calibrating a regression model between the input (e.g., the deformation gradient) and the output (say, a stress measure), we aim to directly generate samples from the input-output joint probability measure, and to estimate quantities of interest through conditional means. This viewpoint requires the use of a generative model capable of accurately capturing measure concentration and the (unknown) geometry of the support of the measure in the small data limit --- a task that remains particularly challenging for strongly non-Gaussian distributions in high dimensions. We note that the construction of generative models is a vibrant topic across many scientific communities, and providing an extensive review on existing techniques is beyond the scope of this paper. In the present study, we employ probabilistic learning on manifolds (PLoM) \cite{Soize2016,Soize2020c,Soize2022a} to perform this task. The choice of this technique is motivated by (\textit{i}) its capability to sample the probability measure defined by the training dataset and in particular, to respect measure concentration and support information (as demonstrated in \cite{Farhat2019,Ghanem2019, Arnst2021,Capiez2022,Ghanem2022, Safta2022, Sinha2023, Zhong2023, Ezvan2023, Almeida2023,Govindjee2023,Soize2024}), (\textit{ii}) relative ease of implementation, and (\textit{iii}) its reliance on low-dimensional, interpretable parameterization. Our main contributions are as follows. First, we formulate the approximation of the non-local homogenized response in nonlinear elasticity as a learning problem. Second, we perform extensive numerical studies and address the validation of the framework under two scenarios relevant to inverse problem solving and forward propagation. In the former case, the approach can be used, for instance, to calibrate hyperparameters in the material model at fine scale, integrating data at the coarse scale. The latter case represents the classical surrogate setting with aleatoric uncertainties induced by subscale randomness (without separation of scales). Notice that while the proposed developments are derived in the context of nonlinear elasticity, they remain applicable to other classes of constitutive models --- at the expense of adapting the mechanistic parameterization.

% This paper is organized as follows. The multiscale mechanistic framework is first introduced in Section \ref{sec:mechanics}. The deterministic scale-coupling problems (and their stochastic counterparts) are presented, together with the stochastic model enabling the representation of material randomness at mesoscale. Section \ref{sec:PLoM} provides a comprehensive overview on the probabilistic learning framework, including both theoretical and algorithmic aspects. In Section \ref{sec:application}, the proposed framework is applied in the context of finite elasticity. The two aforementioned scenarios are specifically introduced to assess the robustness of the method (in probability law). Concluding comments are finally provided in Section \ref{sec:conclusion}.