\chapter{Conclusions}
\label{chap:conclusion}

In Chapter~\ref{chap:artery}, we developed a stochastic model for spatially-dependent anisotropic strain energy density functions. A least-informative model was obtained by applying the maximum entropy principle under constraints related to existence theorems in finite elasticity. This approach therefore ensures that the associated nonlinear boundary value problem is well posed almost surely. Information related to model linearization was also integrated and generate statistical dependencies in the variables parameterizing the stochastic strain energy density function. The identification of the model was performed using a database on human arteries, available in the literature. Here, maximum likelihood estimators were obtained and are provided for the three layers constituting the arterial wall. Finally, uncertainty propagation on a realistic, patient-specific geometry was conducted to demonstrate some capabilities of the stochastic modeling framework.

Avenues for future work include the use of the proposed framework to derive generative models for data-driven methodologies, the integration of the active response exhibited by arteries in in-vivo conditions, as well as refined identification using nondestructive techniques resolving spatial scales.

In Chapter~\ref{chap:polyconvex}, a method to correct unconstrained neural networks for hyperelastic models was proposed. The approach relies on a composite mapping that transforms any function into a convex function, hence ensuring the polyconvexity of the neural network---without constraints on the weights and  activation functions. 

The strategy was first illustrated on a toy problem to characterize the impact of the positive function used to enforce monotonicity (in terms of accuracy and training effort). The rectified NN models were then deployed on digitally synthesized and experimental datasets, relevant to both isotropic and anisotropic materials. Good fitting capabilities were observed in all applications. It was shown that the proposed rectified models typically convergence faster than \textit{a priori} constrained models, at the expense of a greater computational cost per iteration. 

Avenues for future research include the generalization to other types of strain energy density functions, as well as more extensive comparisons with \textit{a priori} constrained representations. 

An operator learning strategy for flow maps exhibiting sharp gradients was proposed. State-of-the-art linear and nonlinear reduction techniques were first reviewed. We then devised a new Convolutional Neural Network (CNN)-based autoencoder. Taking inspiration from adaptive basis methods, the proposed architecture involves iteration-dependent trainable kernels at the decoding stage. Quadratic operator inference and a Deep Neural Network (DNN)-based operator inference model were next investigated to learn the flow map in latent space. We also proposed a strategy to accelerate reduced-order model solving through vectorized implicit time integration. The set of operator learning methods and algorithms thus obtained was benchmarked on an advection-dominated problem (here, a 2D Burgers' equation). It was found that the sparse autoencoder and the proposed CNN-based autoencoder generally perform better in terms of prediction accuracy. It is noticeable however that the latter strategy achieves such performance with much fewer trainable parameters (typically, one order of magnitude less). Regarding operator learning between the latent spaces, quadratic operator inference performed fairly well at the training stage, but was found unstable on the test case. On the contrary, the DNN-based strategy delivered accurate predictions during both training and testing. Finally, it was shown that the vectorized implicit time integration enables substantial speed-ups as the latent space dimension increases. The main limitation of the proposed method lies in time-resolution dependency. A possible way to circumvent this issue is to finite-dimensionalize in time domain, in the spirit of the PCA-Net framework introduced by Stuart and coworkers. Such developments, together with application to other problems presenting non-smooth solution fields, are left for future work.

In Chapter~\ref{chap:fe2}, we have introduced a statistical surrogate model for concurrent multiscale simulations involving nonlinear materials with non-separated scales. The methodology combines probabilistic learning on manifolds, a generative model that allows for measure concentration and support information to be accurately captured, with the use of conditional statistics to approximate the mapping between apparent strain and stress variables --- namely, the right Cauchy-Green tensor and the second Piola-Kirchhoff stress tensor --- at a finite set of points (defining a subregion of interest where concurrent coupling must be deployed). As opposed to standard techniques relying on, e.g., polynomial or neural network surrogates, the proposed approach (1) can readily accommodate the (aleatoric) randomness raised by the multiscale setting, (2) enables the seamless integration of nonlocal interactions through the consideration of joint distributions, and (3) can perform efficiently in the small data regime. Two applications, relevant to inverse problem solving and forward propagation, were presented in the context of nonlinear elasticity. In the first case, the hyperparameters for the prior model defining the random media and boundary conditions at mesoscale are considered as control parameters. This setting can be used, for instance, to identify the hyperparameters when experimental observations are available. In the second application, only mesoscopic boundary displacements are used as control variables. In this case, the prior model at fine scale is fixed, and the effect of material randomness can be quantified. In both cases, large spatial variations, large statistical fluctuations, and strong non-Gaussianity are observed. It was shown that despite these challenges, the framework remains capable of delivering reasonably accurate estimations, even with a fairly limited amount of training data.